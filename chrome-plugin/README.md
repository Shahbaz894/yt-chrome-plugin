chrome-plugin
==============================

we make a chrome plugin which show the commits analysis in details and genrate a summary

Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>


create ec2 instance and also create secret key pair
now connect this instance to console and run thses command
sudo apt-get update
sudo apt-get install python3 python3-venv python3-pip -y
python3 -m venv mlflow_env
<!-- activate envirement -->
source mlflow_env/bin/activate
<!-- install the screen which automaticall start the mlfow server which any issue  -->
pip install mlflow boto3
sudo apt-get install screen -y
<!-- for starting the screen -->
screen -S mlfow

<!-- server run cammand -->
mlflow server --backend-store-uri ./mlruns --default-artifact-root s3://yt-chrome-plugin-bucket --host 0.0.0.0 --port 5000
<!-- for run server on browser -->
<!-- http://ec2-3-27-214-151.ap-southeast-2.compute.amazonaws.com:5000/
http://ec2-3-25-106-164.ap-southeast-2.compute.amazonaws.com/5000 -->
http://ec2-54-252-23-89.ap-southeast-2.compute.amazonaws.com:5000

<!-- to run in vscode intsall thiss -->
!pip install boto3
!pip install awscli
!aws configure
<!-- give follwing parameter -->
AWS Access Key ID [None]: 
AWS Secret Access Key [None]:
Default region name [None]: 
Default output format [None]: 

tep-by-Step Guide: Configuring DVC with AWS S3 as Remote Storage
1Ô∏è‚É£ Create an S3 Bucket
Go to AWS Console ‚Üí S3 ‚Üí Click Create bucket.
Choose a unique name (e.g., campusxproject2bucket).
Select a region and keep default settings.
Click Create bucket.
2Ô∏è‚É£ Create an IAM User (Skip if Already Exists)
Go to AWS IAM Console ‚Üí Users ‚Üí Click Add users.
Enter a username and enable Programmatic access.
Attach "AmazonS3FullAccess" policy.
Create and download the access keys (you‚Äôll need them later).
3Ô∏è‚É£ Install Required Packages
Run the following command in your terminal:


pip install dvc[s3] awscli
dvc[s3]: Adds S3 support to DVC.
awscli: AWS Command Line Interface for authentication.
4Ô∏è‚É£ Remove Existing DVC Remote (If Any)
Run:

dvc remote remove myremote
This removes any previous remote named myremote.
5Ô∏è‚É£ Configure AWS CLI for Authentication
Run:


aws configure
Enter the AWS Access Key and Secret Key from Step 2.
Set region (e.g., us-east-1).
Keep output format as default (press Enter).
6Ô∏è‚É£ Add an S3 Remote to DVC
Run:


dvc remote add -d myremote s3://campusxproject2bucket
myremote: Name of the remote storage.
-d: Sets it as the default DVC remote.
s3://campusxproject2bucket: Path to the S3 bucket.
7Ô∏è‚É£ Add Changes to Git
Run:


git add .
This stages all changes for commit.
8Ô∏è‚É£ Commit Changes to Git
Run:

git commit -m "Configured DVC with S3 remote"
Saves the changes in Git history.
9Ô∏è‚É£ Push Data to S3 Using DVC
Run:

dvc push
Uploads tracked files to S3.
üîü Push Code to GitHub

git push origin main
Pushes the code repository to GitHub.
‚úÖ Final Check
Run dvc remote list to verify the remote is added.
Run dvc status to check if any changes need to be pushed.
Now, your DVC is configured with AWS S3 as remote storage. üöÄ

s3 bucket:
  Amazon Resource Name (ARN)
arn:aws:s3:::yt-chrome-plugin-bucket


If you want to install DVC with S3 support, you should use the following command:


pip install dvc[s3]

dvc remote add -d myremote s3://yt-chrome-plugin-bucket